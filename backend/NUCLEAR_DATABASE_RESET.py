#!/usr/bin/env python3
"""
NUCLEAR DATABASE RESET - CLEAN SLATE SOLUTION
DocXP Enterprise Platform - Fresh Database Creation

Since no data needs to be preserved, this script:
1. Deletes the old database completely
2. Creates a brand new database with correct schema
3. Verifies everything works perfectly

FASTEST AND CLEANEST SOLUTION - No migration needed!
"""

import os
import asyncio
import sys
from pathlib import Path
from datetime import datetime

def log(message, level="INFO"):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

def nuclear_reset():
    """Complete database reset - cleanest solution"""
    
    log("üî• NUCLEAR DATABASE RESET - CLEAN SLATE APPROACH", "INFO")
    log("="*60, "INFO")
    log("Since no data preservation is needed, creating fresh database", "INFO")
    log("="*60, "INFO")
    
    # Step 1: Remove old database files
    database_files = ["docxp.db", "docxp.db-shm", "docxp.db-wal"]
    
    for db_file in database_files:
        if os.path.exists(db_file):
            try:
                os.remove(db_file)
                log(f"‚úÖ Deleted old database file: {db_file}", "SUCCESS")
            except Exception as e:
                log(f"‚ö†Ô∏è  Could not delete {db_file}: {e}", "WARNING")
        else:
            log(f"üìù Database file {db_file} doesn't exist (OK)", "INFO")
    
    # Step 2: Create fresh database using SQLAlchemy
    log("üöÄ Creating fresh database with correct schema...", "INFO")
    
    try:
        # Add app to Python path
        backend_dir = os.path.dirname(os.path.abspath(__file__))
        if backend_dir not in sys.path:
            sys.path.insert(0, backend_dir)
        
        from app.core.database import init_db
        
        async def create_fresh_db():
            await init_db()
            log("‚úÖ Fresh database created with SQLAlchemy", "SUCCESS")
        
        # Run the async database creation
        asyncio.run(create_fresh_db())
        
    except Exception as e:
        log(f"‚ùå SQLAlchemy creation failed: {e}", "ERROR")
        log("üîÑ Falling back to manual database creation...", "INFO")
        return create_database_manually()
    
    # Step 3: Verify the new database
    return verify_database()

def create_database_manually():
    """Manual database creation if SQLAlchemy fails"""
    import sqlite3
    
    try:
        log("üîß Creating database manually...", "INFO")
        
        conn = sqlite3.connect("docxp.db")
        cursor = conn.cursor()
        
        # Create documentation_jobs table with COMPLETE schema
        cursor.execute("""
        CREATE TABLE documentation_jobs (
            id INTEGER PRIMARY KEY,
            job_id VARCHAR UNIQUE,
            repository_path VARCHAR NOT NULL,
            status VARCHAR DEFAULT 'pending',
            created_at DATETIME,
            completed_at DATETIME,
            config JSON NOT NULL,
            progress_percentage INTEGER DEFAULT 0,
            current_step VARCHAR,
            step_description VARCHAR,
            progress_data JSON,
            entities_count INTEGER DEFAULT 0,
            business_rules_count INTEGER DEFAULT 0,
            files_processed INTEGER DEFAULT 0,
            output_path VARCHAR,
            processing_time_seconds FLOAT,
            error_message TEXT
        )
        """)
        
        # Create indexes
        cursor.execute("CREATE UNIQUE INDEX ix_documentation_jobs_job_id ON documentation_jobs (job_id)")
        cursor.execute("CREATE INDEX ix_documentation_jobs_id ON documentation_jobs (id)")
        
        # Create repositories table
        cursor.execute("""
        CREATE TABLE repositories (
            id INTEGER PRIMARY KEY,
            path VARCHAR UNIQUE NOT NULL,
            name VARCHAR NOT NULL,
            last_analyzed DATETIME,
            total_files INTEGER DEFAULT 0,
            total_lines INTEGER DEFAULT 0,
            languages JSON DEFAULT '[]',
            git_remote VARCHAR,
            last_commit VARCHAR,
            created_at DATETIME,
            updated_at DATETIME
        )
        """)
        cursor.execute("CREATE INDEX ix_repositories_id ON repositories (id)")
        
        # Create configuration_templates table
        cursor.execute("""
        CREATE TABLE configuration_templates (
            id INTEGER PRIMARY KEY,
            name VARCHAR UNIQUE NOT NULL,
            description TEXT,
            config JSON NOT NULL,
            is_default BOOLEAN DEFAULT 0,
            created_at DATETIME,
            updated_at DATETIME
        )
        """)
        cursor.execute("CREATE INDEX ix_configuration_templates_id ON configuration_templates (id)")
        
        conn.commit()
        conn.close()
        
        log("‚úÖ Manual database creation successful!", "SUCCESS")
        return True
        
    except Exception as e:
        log(f"‚ùå Manual database creation failed: {e}", "ERROR")
        return False

def verify_database():
    """Verify the new database works perfectly"""
    import sqlite3
    
    try:
        log("üîç Verifying new database...", "INFO")
        
        if not os.path.exists("docxp.db"):
            log("‚ùå Database file was not created!", "ERROR")
            return False
        
        conn = sqlite3.connect("docxp.db")
        cursor = conn.cursor()
        
        # Check documentation_jobs table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='documentation_jobs'")
        if not cursor.fetchone():
            log("‚ùå documentation_jobs table not found!", "ERROR")
            return False
        
        # Check ALL required columns exist (especially the problematic ones)
        cursor.execute("PRAGMA table_info(documentation_jobs)")
        columns = cursor.fetchall()
        column_names = [col[1] for col in columns]
        
        required_columns = [
            'id', 'job_id', 'repository_path', 'status', 'created_at', 'completed_at',
            'config', 'progress_percentage', 'current_step', 'step_description', 
            'progress_data', 'entities_count', 'business_rules_count', 
            'files_processed', 'output_path', 'processing_time_seconds', 'error_message'
        ]
        
        missing = [col for col in required_columns if col not in column_names]
        
        if missing:
            log(f"‚ùå Missing columns: {missing}", "ERROR")
            return False
        
        log("‚úÖ All required columns present!", "SUCCESS")
        
        # Test the EXACT query that was failing before
        cursor.execute("""
        SELECT documentation_jobs.id, documentation_jobs.job_id, documentation_jobs.repository_path, 
               documentation_jobs.status, documentation_jobs.created_at, documentation_jobs.completed_at, 
               documentation_jobs.config, documentation_jobs.progress_percentage, 
               documentation_jobs.current_step, documentation_jobs.step_description, 
               documentation_jobs.progress_data, documentation_jobs.entities_count, 
               documentation_jobs.business_rules_count, documentation_jobs.files_processed, 
               documentation_jobs.output_path, documentation_jobs.processing_time_seconds, 
               documentation_jobs.error_message
        FROM documentation_jobs 
        ORDER BY documentation_jobs.created_at DESC 
        LIMIT 10 OFFSET 0
        """)
        
        log("‚úÖ Complex query test PASSED!", "SUCCESS")
        
        # Test analytics query that was also failing
        cursor.execute("""
        SELECT documentation_jobs.id, documentation_jobs.job_id, documentation_jobs.repository_path, 
               documentation_jobs.status, documentation_jobs.created_at, documentation_jobs.completed_at, 
               documentation_jobs.config, documentation_jobs.progress_percentage, 
               documentation_jobs.current_step, documentation_jobs.step_description, 
               documentation_jobs.progress_data, documentation_jobs.entities_count, 
               documentation_jobs.business_rules_count, documentation_jobs.files_processed, 
               documentation_jobs.output_path, documentation_jobs.processing_time_seconds, 
               documentation_jobs.error_message
        FROM documentation_jobs
        WHERE documentation_jobs.created_at >= datetime('now', '-30 days')
        ORDER BY documentation_jobs.created_at
        """)
        
        log("‚úÖ Analytics query test PASSED!", "SUCCESS")
        
        conn.close()
        
        log("="*60, "INFO")
        log("üéâ DATABASE VERIFICATION SUCCESSFUL!", "SUCCESS")
        log("‚úÖ Fresh database is ready for DocXP!", "SUCCESS")
        log("="*60, "INFO")
        
        return True
        
    except Exception as e:
        log(f"‚ùå Database verification failed: {e}", "ERROR")
        return False

def main():
    """Main execution"""
    print("DocXP NUCLEAR DATABASE RESET")
    print("="*40)
    print("üî• CLEAN SLATE APPROACH - No data preservation needed")
    print("This will completely delete and recreate the database.")
    print("")
    
    try:
        success = nuclear_reset()
        
        if success:
            print("\nüéâ SUCCESS! Fresh database created and verified!")
            print("\n‚úÖ NEXT STEPS:")
            print("1. Start your DocXP application: python main.py")
            print("2. All database errors are now PERMANENTLY FIXED")
            print("3. /api/documentation/jobs endpoint will work perfectly")
            print("4. /api/analytics/trends endpoint will work perfectly") 
            print("5. You can now generate documentation without issues")
            print("\nüöÄ Your DocXP Enterprise Platform is ready!")
        else:
            print("\n‚ùå RESET FAILED!")
            print("Please check the error messages above.")
        
        return success
        
    except Exception as e:
        print(f"\nüí• CRITICAL ERROR: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)